{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import packages and select GPU if accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "from st_loading_utils import load_DLPFC, load_BC, load_mVC, load_mPFC, load_mHypothalamus, load_her2_tumor, load_mMAMP\n",
    "from model import SpaCLR, TrainerSpaCLR\n",
    "from utils import get_predicted_results, load_ST_file\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from dataset import Dataset\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# preprocess\n",
    "parser.add_argument('--dataset', type=str, default=\"SpatialLIBD\")\n",
    "parser.add_argument('--path', type=str, default=\"../spatialLIBD\")\n",
    "parser.add_argument(\"--gene_preprocess\", choices=(\"pca\", \"hvg\"), default=\"pca\")\n",
    "parser.add_argument(\"--n_gene\", choices=(300, 1000), default=300)\n",
    "parser.add_argument('--img_size', type=int, default=112)\n",
    "parser.add_argument('--num_workers', type=int, default=8)\n",
    "\n",
    "# model\n",
    "parser.add_argument('--last_dim', type=int, default=64)\n",
    "parser.add_argument('--lr', type=float, default=0.0003)\n",
    "parser.add_argument('--p_drop', type=float, default=0)\n",
    "\n",
    "parser.add_argument('--w_g2i', type=float, default=1)\n",
    "parser.add_argument('--w_g2g', type=float, default=0.1)\n",
    "parser.add_argument('--w_i2i', type=float, default=0.1)\n",
    "parser.add_argument('--w_recon', type=float, default=0)\n",
    "\n",
    "# data augmentation\n",
    "parser.add_argument('--prob_mask', type=float, default=0.5)\n",
    "parser.add_argument('--pct_mask', type=float, default=0.2)\n",
    "parser.add_argument('--prob_noise', type=float, default=0.5)\n",
    "parser.add_argument('--pct_noise', type=float, default=0.8)\n",
    "parser.add_argument('--sigma_noise', type=float, default=0.5)\n",
    "parser.add_argument('--prob_swap', type=float, default=0.5)\n",
    "parser.add_argument('--pct_swap', type=float, default=0.1)\n",
    "\n",
    "# train\n",
    "parser.add_argument('--batch_size', type=int, default=96)\n",
    "parser.add_argument('--epochs', type=int, default=35)\n",
    "parser.add_argument('--device', type=str, default=\"cuda:3\")\n",
    "parser.add_argument('--log_name', type=str, default=\"log_name\")\n",
    "parser.add_argument('--name', type=str, default=\"None\")\n",
    "\n",
    "iters=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DLPFC dataset (12 slides)\n",
    "\n",
    "change '${dir_}' to  'path/to/your/DLPFC/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DLPFC\"\"\"\n",
    "# the number of clusters\n",
    "setting_combinations = [[7, '151674'], [7, '151675'], [7, '151676']] [7, '151507'], [7, '151508'], [7, '151509'], [7, '151510'], [5, '151669'], [5, '151670'], [5, '151671'], [5, '151672'], [7, '151673'], [7, '151674'], [7, '151675'],[7, '151676']]\n",
    "# setting_combinations = [ \n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = '/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12'\n",
    "    name = args.name = setting_combi[1]\n",
    "    gene_preprocess = args.gene_preprocess\n",
    "    n_gene = args.n_gene\n",
    "    last_dim = args.last_dim\n",
    "    gene_dims=[n_gene, 2*last_dim]\n",
    "    image_dims=[n_gene]\n",
    "    lr = args.lr\n",
    "    p_drop = args.p_drop\n",
    "    batch_size = args.batch_size\n",
    "    dataset = args.dataset = 'DLPFC'\n",
    "    epochs = args.epochs\n",
    "    img_size = args.img_size\n",
    "    device = args.device\n",
    "    log_name = args.log_name\n",
    "    num_workers = args.num_workers\n",
    "    prob_mask = args.prob_mask\n",
    "    pct_mask = args.pct_mask\n",
    "    prob_noise = args.prob_noise\n",
    "    pct_noise = args.pct_noise\n",
    "    sigma_noise = args.sigma_noise\n",
    "    prob_swap = args.prob_swap\n",
    "    pct_swap = args.pct_swap\n",
    "    aris = []\n",
    "    for iter_ in range(iters):\n",
    "        # dataset\n",
    "        trainset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=True)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        testset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=False)\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # network\n",
    "        network = SpaCLR(gene_dims=gene_dims, image_dims=image_dims, p_drop=p_drop, n_pos=trainset.n_pos, backbone='densenet', projection_dims=[last_dim, last_dim])\n",
    "        optimizer = torch.optim.AdamW(network.parameters(), lr=lr)\n",
    "\n",
    "        # log\n",
    "        save_name = f'{name}_{args.w_g2i}_{args.w_g2g}_{args.w_i2i}'\n",
    "        log_dir = os.path.join('log', log_name, save_name)\n",
    "\n",
    "        # train\n",
    "        trainer = TrainerSpaCLR(args, trainset.n_clusters, network, optimizer, log_dir, device=device)\n",
    "        trainer.fit(trainloader, epochs)\n",
    "        xg, xi, _ = trainer.valid(testloader)\n",
    "        z = xg + 0.1*xi\n",
    "\n",
    "        ARI, pred_label = get_predicted_results(args.dataset, args.name, args.path, z)\n",
    "        print(\"Ari value : \", ARI)\n",
    "\n",
    "        print('Dataset:', name)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', name)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('congi_aris.txt', 'a+') as fp:\n",
    "        fp.write('DLPFC' + name + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BC/MA datasets (2 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BC\"\"\"\n",
    "# the number of clusters\n",
    "setting_combinations = [[20, 'section1']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = '/home/yunfei/spatial_benchmarking/benchmarking_data/BC'\n",
    "    name = args.name = setting_combi[1]\n",
    "    gene_preprocess = args.gene_preprocess\n",
    "    n_gene = args.n_gene\n",
    "    last_dim = args.last_dim\n",
    "    gene_dims=[n_gene, 2*last_dim]\n",
    "    image_dims=[n_gene]\n",
    "    lr = args.lr\n",
    "    p_drop = args.p_drop\n",
    "    batch_size = args.batch_size\n",
    "    dataset = args.dataset = 'BC'\n",
    "    epochs = args.epochs\n",
    "    img_size = args.img_size\n",
    "    device = args.device\n",
    "    log_name = args.log_name\n",
    "    num_workers = args.num_workers\n",
    "    prob_mask = args.prob_mask\n",
    "    pct_mask = args.pct_mask\n",
    "    prob_noise = args.prob_noise\n",
    "    pct_noise = args.pct_noise\n",
    "    sigma_noise = args.sigma_noise\n",
    "    prob_swap = args.prob_swap\n",
    "    pct_swap = args.pct_swap\n",
    "    aris = []\n",
    "    for iter_ in range(iters):\n",
    "        # dataset\n",
    "        trainset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=True)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        testset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=False)\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # network\n",
    "        network = SpaCLR(gene_dims=gene_dims, image_dims=image_dims, p_drop=p_drop, n_pos=trainset.n_pos, backbone='densenet', projection_dims=[last_dim, last_dim])\n",
    "        optimizer = torch.optim.AdamW(network.parameters(), lr=lr)\n",
    "\n",
    "        # log\n",
    "        save_name = f'{name}_{args.w_g2i}_{args.w_g2g}_{args.w_i2i}'\n",
    "        log_dir = os.path.join('log', log_name, save_name)\n",
    "\n",
    "        # train\n",
    "        trainer = TrainerSpaCLR(args, trainset.n_clusters, network, optimizer, log_dir, device=device)\n",
    "        trainer.fit(trainloader, epochs)\n",
    "        xg, xi, _ = trainer.valid(testloader)\n",
    "        z = xg + 0.1*xi\n",
    "\n",
    "        ARI, pred_label = get_predicted_results(args.dataset, args.name, args.path, z)\n",
    "        print(\"Ari value : \", ARI)\n",
    "\n",
    "        print('Dataset:', name)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', name)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('congi_aris.txt', 'a+') as fp:\n",
    "        fp.write('BC' + name + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MA\"\"\"\n",
    "# the number of clusters\n",
    "setting_combinations = [[52, 'MA']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = '/home/yunfei/spatial_benchmarking/benchmarking_data/mMAMP'\n",
    "    name = args.name = setting_combi[1]\n",
    "    gene_preprocess = args.gene_preprocess\n",
    "    n_gene = args.n_gene\n",
    "    last_dim = args.last_dim\n",
    "    gene_dims=[n_gene, 2*last_dim]\n",
    "    image_dims=[n_gene]\n",
    "    lr = args.lr\n",
    "    p_drop = args.p_drop\n",
    "    batch_size = args.batch_size\n",
    "    dataset = args.dataset = 'MA'\n",
    "    epochs = args.epochs\n",
    "    img_size = args.img_size\n",
    "    device = args.device\n",
    "    log_name = args.log_name\n",
    "    num_workers = args.num_workers\n",
    "    prob_mask = args.prob_mask\n",
    "    pct_mask = args.pct_mask\n",
    "    prob_noise = args.prob_noise\n",
    "    pct_noise = args.pct_noise\n",
    "    sigma_noise = args.sigma_noise\n",
    "    prob_swap = args.prob_swap\n",
    "    pct_swap = args.pct_swap\n",
    "    aris = []\n",
    "    for iter_ in range(iters):\n",
    "        # dataset\n",
    "        trainset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=True)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        testset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=False)\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # network\n",
    "        network = SpaCLR(gene_dims=gene_dims, image_dims=image_dims, p_drop=p_drop, n_pos=trainset.n_pos, backbone='densenet', projection_dims=[last_dim, last_dim])\n",
    "        optimizer = torch.optim.AdamW(network.parameters(), lr=lr)\n",
    "\n",
    "        # log\n",
    "        save_name = f'{name}_{args.w_g2i}_{args.w_g2g}_{args.w_i2i}'\n",
    "        log_dir = os.path.join('log', log_name, save_name)\n",
    "\n",
    "        # train\n",
    "        trainer = TrainerSpaCLR(args, trainset.n_clusters, network, optimizer, log_dir, device=device)\n",
    "        trainer.fit(trainloader, epochs)\n",
    "        xg, xi, _ = trainer.valid(testloader)\n",
    "        z = xg + 0.1*xi\n",
    "\n",
    "        ARI, pred_label = get_predicted_results(args.dataset, args.name, args.path, z)\n",
    "        print(\"Ari value : \", ARI)\n",
    "\n",
    "        print('Dataset:', name)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', name)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('congi_aris.txt', 'a+') as fp:\n",
    "        fp.write('mAB' + name + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Her2Tumor dataset (8 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Her2st\"\"\"\n",
    "# the number of clusters [6, 'A1'], [5, 'B1'], [4, 'C1'], \n",
    "setting_combinations = [[4, 'D1'], [4, 'E1'], [4, 'F1'], [7, 'G2'], [7, 'H1']]\n",
    "# setting_combinations = [[7, '151674'], [7, '151675'], [7, '151676']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = '/home/yunfei/spatial_benchmarking/benchmarking_data/Her2_tumor'\n",
    "    name = args.name = setting_combi[1]\n",
    "    gene_preprocess = args.gene_preprocess\n",
    "    n_gene = args.n_gene\n",
    "    last_dim = args.last_dim\n",
    "    gene_dims=[n_gene, 2*last_dim]\n",
    "    image_dims=[n_gene]\n",
    "    lr = args.lr\n",
    "    p_drop = args.p_drop\n",
    "    batch_size = args.batch_size\n",
    "    dataset = args.dataset = 'Her2st'\n",
    "    epochs = args.epochs\n",
    "    img_size = args.img_size\n",
    "    device = args.device\n",
    "    log_name = args.log_name\n",
    "    num_workers = args.num_workers\n",
    "    prob_mask = args.prob_mask\n",
    "    pct_mask = args.pct_mask\n",
    "    prob_noise = args.prob_noise\n",
    "    pct_noise = args.pct_noise\n",
    "    sigma_noise = args.sigma_noise\n",
    "    prob_swap = args.prob_swap\n",
    "    pct_swap = args.pct_swap\n",
    "    aris = []\n",
    "    for iter_ in range(iters):\n",
    "        # dataset\n",
    "        trainset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=True)\n",
    "        trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        testset = Dataset(dataset, path, name, gene_preprocess=gene_preprocess, n_genes=n_gene,\n",
    "                        prob_mask=prob_mask, pct_mask=pct_mask, prob_noise=prob_noise, pct_noise=pct_noise, sigma_noise=sigma_noise,\n",
    "                        prob_swap=prob_swap, pct_swap=pct_swap, img_size=img_size, train=False)\n",
    "        testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "        # network\n",
    "        network = SpaCLR(gene_dims=gene_dims, image_dims=image_dims, p_drop=p_drop, n_pos=trainset.n_pos, backbone='densenet', projection_dims=[last_dim, last_dim])\n",
    "        optimizer = torch.optim.AdamW(network.parameters(), lr=lr)\n",
    "\n",
    "        # log\n",
    "        save_name = f'{name}_{args.w_g2i}_{args.w_g2g}_{args.w_i2i}'\n",
    "        log_dir = os.path.join('log', log_name, save_name)\n",
    "\n",
    "        # train\n",
    "        trainer = TrainerSpaCLR(args, trainset.n_clusters, network, optimizer, log_dir, device=device)\n",
    "        trainer.fit(trainloader, epochs)\n",
    "        xg, xi, _ = trainer.valid(testloader)\n",
    "        z = xg + 0.1*xi\n",
    "\n",
    "        ARI, pred_label = get_predicted_results(args.dataset, args.name, args.path, z)\n",
    "        print(\"Ari value : \", ARI)\n",
    "\n",
    "        print('Dataset:', name)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', name)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('congi_aris.txt', 'a+') as fp:\n",
    "        fp.write('Her2tumor' + name + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
