{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from sklearn import metrics\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import squidpy as sq\n",
    "import scanpy as sc\n",
    "from SpaceFlow import SpaceFlow\n",
    "from st_loading_utils import load_DLPFC, load_BC, load_mVC, load_mPFC, load_mHypothalamus, load_her2_tumor, load_mMAMP, load_embryo\n",
    "from scipy.spatial import *\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from scipy.spatial.distance import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_search_fixed_clus(adata, fixed_clus_count, increment=0.1):\n",
    "    '''\n",
    "        arg1(adata)[AnnData matrix]\n",
    "        arg2(fixed_clus_count)[int]\n",
    "        \n",
    "        return:\n",
    "            resolution[int]\n",
    "    '''\n",
    "    for res in sorted(list(np.arange(0.2, 2.5, increment)), reverse=True):\n",
    "        sc.tl.leiden(adata, random_state=0, resolution=res)\n",
    "        count_unique_leiden = len(pd.DataFrame(adata.obs['leiden']).leiden.unique())\n",
    "        if count_unique_leiden == fixed_clus_count:\n",
    "            break\n",
    "    return res\n",
    "\n",
    "\n",
    "def fx_1NN(i,location_in):\n",
    "    location_in = np.array(location_in)\n",
    "    dist_array = distance_matrix(location_in[i,:][None,:],location_in)[0,:]\n",
    "    dist_array[i] = np.inf\n",
    "    return np.min(dist_array)\n",
    "\n",
    "\n",
    "def fx_kNN(i,location_in,k,cluster_in):\n",
    "\n",
    "    location_in = np.array(location_in)\n",
    "    cluster_in = np.array(cluster_in)\n",
    "\n",
    "\n",
    "    dist_array = distance_matrix(location_in[i,:][None,:],location_in)[0,:]\n",
    "    dist_array[i] = np.inf\n",
    "    ind = np.argsort(dist_array)[:k]\n",
    "    cluster_use = np.array(cluster_in)\n",
    "    if np.sum(cluster_use[ind]!=cluster_in[i])>(k/2):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def _compute_CHAOS(clusterlabel, location):\n",
    "\n",
    "    clusterlabel = np.array(clusterlabel)\n",
    "    location = np.array(location)\n",
    "    matched_location = StandardScaler().fit_transform(location)\n",
    "\n",
    "    clusterlabel_unique = np.unique(clusterlabel)\n",
    "    dist_val = np.zeros(len(clusterlabel_unique))\n",
    "    count = 0\n",
    "    for k in clusterlabel_unique:\n",
    "        location_cluster = matched_location[clusterlabel==k,:]\n",
    "        if len(location_cluster)<=2:\n",
    "            continue\n",
    "        n_location_cluster = len(location_cluster)\n",
    "        results = [fx_1NN(i,location_cluster) for i in range(n_location_cluster)]\n",
    "        dist_val[count] = np.sum(results)\n",
    "        count = count + 1\n",
    "\n",
    "    return np.sum(dist_val)/len(clusterlabel)\n",
    "\n",
    "def _compute_PAS(clusterlabel,location):\n",
    "        \n",
    "    clusterlabel = np.array(clusterlabel)\n",
    "    location = np.array(location)\n",
    "    matched_location = location\n",
    "    results = [fx_kNN(i,matched_location,k=10,cluster_in=clusterlabel) for i in range(matched_location.shape[0])]\n",
    "    return np.sum(results)/len(clusterlabel)\n",
    "\n",
    "\n",
    "def compute_CHAOS(adata,pred_key,spatial_key='spatial'):\n",
    "    return _compute_CHAOS(adata.obs[pred_key],adata.obsm[spatial_key])\n",
    "\n",
    "def compute_PAS(adata,pred_key,spatial_key='spatial'):\n",
    "    return _compute_PAS(adata.obs[pred_key],adata.obsm[spatial_key])\n",
    "\n",
    "def compute_ASW(adata,pred_key,spatial_key='spatial'):\n",
    "    d = squareform(pdist(adata.obsm[spatial_key]))\n",
    "    return silhouette_score(X=d,labels=adata.obs[pred_key],metric='precomputed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLPFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DLPFC\"\"\"\n",
    "setting_combinations = [[7, '151507'], [7, '151508'], [7, '151509'], [7, '151510'], [5, '151669'], [5, '151670'], [5, '151671'], [5, '151672'], [7, '151673'], [7, '151674'], [7, '151675'], [7, '151676']]\n",
    "\n",
    "for setting_combi in setting_combinations:\n",
    "    n_clusters = setting_combi[0]  # 7\n",
    "\n",
    "    dataset = setting_combi[1]  # '151673'\n",
    "\n",
    "    dir_ = '../benchmarking_data/DLPFC12'\n",
    "    adata = load_DLPFC(root_dir=dir_, section_id=dataset)\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    adata.var_names_make_unique()\n",
    "    sf = SpaceFlow.SpaceFlow(adata=adata)\n",
    "\n",
    "    #preprocess\n",
    "    sf.preprocessing_data(n_top_genes=3000)\n",
    "    \n",
    "    ari_list = []\n",
    "    nmi_list = []\n",
    "    ami_list = []\n",
    "    hm_list = []\n",
    "    time_list = []\n",
    "    chaos_list = []\n",
    "    pas_list = []\n",
    "    asw_list = []\n",
    "    for iter in range(20):\n",
    "        import tracemalloc\n",
    "        import time\n",
    "    \n",
    "        tracemalloc.start()  \n",
    "        start_time=time.time()\n",
    "\n",
    "        sf.train(spatial_regularization_strength=0.1, \n",
    "            embedding_save_filepath=\"./results_0424/DLPFC/\"+dataset+\"_\"+str(iter)+\"embedding.tsv\",\n",
    "            z_dim=50, \n",
    "            lr=1e-3, \n",
    "            epochs=1000, \n",
    "            max_patience=50, \n",
    "            min_stop=100, \n",
    "            random_seed=42, \n",
    "            gpu=1, \n",
    "            regularization_acceleration=True, \n",
    "            edge_subset_sz=1000000)\n",
    "        \n",
    "        # n_clusters=7\n",
    "        sc.pp.neighbors(adata, n_neighbors=50)\n",
    "        eval_resolution = res_search_fixed_clus(adata, n_clusters)\n",
    "\n",
    "        sf.segmentation(domain_label_save_filepath=\"./results_0424/DLPFC/\"+dataset+\"_\"+str(iter)+\"domains.tsv\".format(iter+1), \n",
    "                    n_neighbors=50, \n",
    "                    resolution=eval_resolution)\n",
    "        \n",
    "        pred=pd.read_csv(\"./results_0424/DLPFC/\"+dataset+\"_\"+str(iter)+\"domains.tsv\".format(iter+1),header=None)\n",
    "        pred_list=pred.iloc[:,0].to_list()\n",
    "        \n",
    "        adata.obs['pred_{}'.format(iter+1)] = np.array(pred_list)\n",
    "\n",
    "        obs_df = adata.obs.dropna()\n",
    "        ari = metrics.adjusted_rand_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        nmi = metrics.normalized_mutual_info_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        # print(\"AMI\")\n",
    "        ami = metrics.adjusted_mutual_info_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        # print(\"homogeneity\")\n",
    "        homogeneity = metrics.homogeneity_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "\n",
    "        chaos = compute_CHAOS(adata, 'pred_{}'.format(iter+1))\n",
    "        pas = compute_PAS(adata, 'pred_{}'.format(iter+1))\n",
    "        asw = compute_ASW(adata, 'pred_{}'.format(iter+1))\n",
    "\n",
    "\n",
    "        ari_list.append(ari)\n",
    "        nmi_list.append(nmi)\n",
    "        ami_list.append(ami)\n",
    "        hm_list.append(homogeneity)\n",
    "        chaos_list.append(chaos)\n",
    "        pas_list.append(pas)\n",
    "        asw_list.append(asw)\n",
    "\n",
    "        end_time=time.time()\n",
    "        during=end_time-start_time\n",
    "\n",
    "        size, peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "        tracemalloc.stop()\n",
    "        \n",
    "        # memory[i]=peak /1024/1024\n",
    "        time_list.append(during)\n",
    "        \n",
    "    \n",
    "        # print('memory blocks peak:{:>10.4f} MB'.format(memory[i]))\n",
    "        print('time: {:.4f} s'.format(during))\n",
    "        print('ARI:{}'.format(ari))\n",
    "        print('NMI:{}'.format(nmi))\n",
    "        print('AMI:{}'.format(ami))\n",
    "        print('Homogeneity:{}'.format(homogeneity))\n",
    "        print('chaos:{}'.format(chaos))\n",
    "        print('pas:{}'.format(pas))\n",
    "        print('asw:{}'.format(asw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MHypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MHypo\"\"\"\n",
    "setting_combinations = [[8, '-0.04'], [8, '-0.09'], [8, '-0.14'], [8, '-0.19'], [8, '-0.24']]\n",
    "for setting_combi in setting_combinations:\n",
    "    n_clusters = setting_combi[0]  # 7\n",
    "\n",
    "    dataset = setting_combi[1]  # '151673'\n",
    "\n",
    "    dir_ = '../benchmarking_data/mHypothalamus'\n",
    "    adata = load_mHypothalamus(root_dir=dir_, section_id=dataset)\n",
    "    sc.pp.filter_genes(adata, min_cells=3)\n",
    "    adata.var_names_make_unique()\n",
    "    sf = SpaceFlow.SpaceFlow(adata=adata)\n",
    "\n",
    "    #preprocess\n",
    "    sf.preprocessing_data(n_top_genes=3000)\n",
    "    \n",
    "    ari_list = []\n",
    "    nmi_list = []\n",
    "    ami_list = []\n",
    "    hm_list = []\n",
    "    time_list = []\n",
    "    chaos_list = []\n",
    "    pas_list = []\n",
    "    asw_list = []\n",
    "    for iter in range(20):\n",
    "        import tracemalloc\n",
    "        import time\n",
    "    \n",
    "        tracemalloc.start()  \n",
    "        start_time=time.time()\n",
    "\n",
    "        sf.train(spatial_regularization_strength=0.1, \n",
    "            embedding_save_filepath=\"./results_0424/mHypo/\"+dataset+\"_\"+str(iter)+\"embedding.tsv\",\n",
    "            z_dim=50, \n",
    "            lr=1e-3, \n",
    "            epochs=1000, \n",
    "            max_patience=50, \n",
    "            min_stop=100, \n",
    "            random_seed=42, \n",
    "            gpu=1, \n",
    "            regularization_acceleration=True, \n",
    "            edge_subset_sz=1000000)\n",
    "        \n",
    "        # n_clusters=7\n",
    "        sc.pp.neighbors(adata, n_neighbors=50)\n",
    "        eval_resolution = res_search_fixed_clus(adata, n_clusters)\n",
    "\n",
    "        sf.segmentation(domain_label_save_filepath=\"./results_0424/mHypo/\"+dataset+\"_\"+str(iter)+\"domains.tsv\".format(iter+1), \n",
    "                    n_neighbors=50, \n",
    "                    resolution=eval_resolution)\n",
    "        \n",
    "        pred=pd.read_csv(\"./results_0424/mHypo/\"+dataset+\"_\"+str(iter)+\"domains.tsv\".format(iter+1),header=None)\n",
    "        pred_list=pred.iloc[:,0].to_list()\n",
    "        \n",
    "        adata.obs['pred_{}'.format(iter+1)] = np.array(pred_list)\n",
    "\n",
    "        obs_df = adata.obs.dropna()\n",
    "        ari = metrics.adjusted_rand_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        nmi = metrics.normalized_mutual_info_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        # print(\"AMI\")\n",
    "        ami = metrics.adjusted_mutual_info_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "        # print(\"homogeneity\")\n",
    "        homogeneity = metrics.homogeneity_score(obs_df['original_clusters'], obs_df['pred_{}'.format(iter+1)])\n",
    "\n",
    "        chaos = compute_CHAOS(adata, 'pred_{}'.format(iter+1))\n",
    "        pas = compute_PAS(adata, 'pred_{}'.format(iter+1))\n",
    "        asw = compute_ASW(adata, 'pred_{}'.format(iter+1))\n",
    "\n",
    "\n",
    "        ari_list.append(ari)\n",
    "        nmi_list.append(nmi)\n",
    "        ami_list.append(ami)\n",
    "        hm_list.append(homogeneity)\n",
    "        chaos_list.append(chaos)\n",
    "        pas_list.append(pas)\n",
    "        asw_list.append(asw)\n",
    "\n",
    "        end_time=time.time()\n",
    "        during=end_time-start_time\n",
    "\n",
    "        size, peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "        tracemalloc.stop()\n",
    "        \n",
    "        # memory[i]=peak /1024/1024\n",
    "        time_list.append(during)\n",
    "        \n",
    "    \n",
    "        # print('memory blocks peak:{:>10.4f} MB'.format(memory[i]))\n",
    "        print('time: {:.4f} s'.format(during))\n",
    "        print('ARI:{}'.format(ari))\n",
    "        print('NMI:{}'.format(nmi))\n",
    "        print('AMI:{}'.format(ami))\n",
    "        print('Homogeneity:{}'.format(homogeneity))\n",
    "        print('chaos:{}'.format(chaos))\n",
    "        print('pas:{}'.format(pas))\n",
    "        print('asw:{}'.format(asw))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
