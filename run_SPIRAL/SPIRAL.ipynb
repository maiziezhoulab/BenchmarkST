{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPIRAL integration tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preparation (use DLPFC as an example here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from st_loading_utils import load_DLPFC, load_mHypothalamus\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import anndata\n",
    "import scipy as sp\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cal_Spatial_Net(adata, rad_cutoff=None, k_cutoff=None, model='Radius', verbose=True):\n",
    "    \"\"\"\\\n",
    "    Construct the spatial neighbor networks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object of scanpy package.\n",
    "    rad_cutoff\n",
    "        radius cutoff when model='Radius'\n",
    "    k_cutoff\n",
    "        The number of nearest neighbors when model='KNN'\n",
    "    model\n",
    "        The network construction model. When model=='Radius', the spot is connected to spots whose distance is less than rad_cutoff. When model=='KNN', the spot is connected to its first k_cutoff nearest neighbors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The spatial networks are saved in adata.uns['Spatial_Net']\n",
    "    \"\"\"\n",
    "\n",
    "    assert(model in ['Radius', 'KNN'])\n",
    "    if verbose:\n",
    "        print('------Calculating spatial graph...')\n",
    "    coor = pd.DataFrame(adata.obsm['spatial'])\n",
    "    coor.index = adata.obs.index\n",
    "#     coor.columns = ['imagerow', 'imagecol']\n",
    "\n",
    "    if model == 'Radius':\n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(radius=rad_cutoff).fit(coor)\n",
    "        distances, indices = nbrs.radius_neighbors(coor, return_distance=True)\n",
    "        KNN_list = []\n",
    "        for it in range(indices.shape[0]):\n",
    "            KNN_list.append(pd.DataFrame(zip([it]*indices[it].shape[0], indices[it], distances[it])))\n",
    "    \n",
    "    if model == 'KNN':\n",
    "        nbrs = sklearn.neighbors.NearestNeighbors(n_neighbors=k_cutoff+1).fit(coor)\n",
    "        distances, indices = nbrs.kneighbors(coor)\n",
    "        KNN_list = []\n",
    "        for it in range(indices.shape[0]):\n",
    "            KNN_list.append(pd.DataFrame(zip([it]*indices.shape[1],indices[it,:], distances[it,:])))\n",
    "\n",
    "    KNN_df = pd.concat(KNN_list)\n",
    "    KNN_df.columns = ['Cell1', 'Cell2', 'Distance']\n",
    "\n",
    "    Spatial_Net = KNN_df.copy()\n",
    "    Spatial_Net = Spatial_Net.loc[Spatial_Net['Distance']>0,]\n",
    "    id_cell_trans = dict(zip(range(coor.shape[0]), np.array(coor.index), ))\n",
    "    Spatial_Net['Cell1'] = Spatial_Net['Cell1'].map(id_cell_trans)\n",
    "    Spatial_Net['Cell2'] = Spatial_Net['Cell2'].map(id_cell_trans)\n",
    "    if verbose:\n",
    "        print('The graph contains %d edges, %d cells.' %(Spatial_Net.shape[0], adata.n_obs))\n",
    "        print('%.4f neighbors per cell on average.' %(Spatial_Net.shape[0]/adata.n_obs))\n",
    "\n",
    "    adata.uns['Spatial_Net'] = Spatial_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs=\"/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12\"\n",
    "\n",
    "# lists=[[151507,151508],[151508,151509],[151509,151510],[151669,151670],[151670,151671],[151671,151672],[151673,151674],[151674,151675],[151675,151676]]\n",
    "lists=[[151507, 151508, 151509, 151510], [151669, 151670, 151671, 151672], [151673, 151674, 151675, 151676]]\n",
    "out_dirs=\"./data/DLPFC/\"\n",
    "\n",
    "for sample_name in lists:\n",
    "    # IDX=np.arange(0,2)\n",
    "    IDX=np.arange(0,4)\n",
    "    VF=[]\n",
    "    MAT=[]\n",
    "    flags=str(sample_name[IDX[0]])\n",
    "    for i in np.arange(1,len(IDX)):\n",
    "        flags=flags+'-'+str(sample_name[IDX[i]])\n",
    "    flags=flags+\"_\"\n",
    "    for k in np.arange(len(IDX)):\n",
    "        adata = load_DLPFC(root_dir=dirs, section_id=str(sample_name[k]))\n",
    "        adata.var_names_make_unique()\n",
    "\n",
    "        sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "        sc.pp.log1p(adata)\n",
    "\n",
    "        adata.obs['batch']=str(sample_name[IDX[k]])\n",
    "        cells=[str(sample_name[IDX[k]])+'-'+i for i in adata.obs_names]\n",
    "        mat1=pd.DataFrame(adata.X.toarray(),columns=adata.var_names,index=cells)\n",
    "        coord1=pd.DataFrame(adata.obsm['spatial'],columns=['x','y'],index=cells)\n",
    "        meta1=adata.obs[['original_clusters', 'batch']]\n",
    "        meta1.columns=['celltype','batch']\n",
    "        meta1.index=cells\n",
    "        meta1.to_csv(out_dirs+\"gtt_input_scanpy/\"+flags+str(sample_name[IDX[k]])+\"_label-1.txt\")\n",
    "        coord1.to_csv(out_dirs+\"gtt_input_scanpy/\"+flags+str(sample_name[IDX[k]])+\"_positions-1.txt\")\n",
    "        MAT.append(mat1)\n",
    "        VF=np.union1d(VF,adata.var_names[adata.var['highly_variable']])\n",
    "\n",
    "    for i in np.arange(len(IDX)):\n",
    "        mat=MAT[i]\n",
    "        mat=mat.loc[:,VF]\n",
    "        mat.to_csv(out_dirs+\"gtt_input_scanpy/\"+flags+str(sample_name[IDX[i]])+\"_features-1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad=150\n",
    "KNN=6\n",
    "\n",
    "# lists=[[151507,151508],[151508,151509],[151509,151510],[151669,151670],[151670,151671],[151671,151672],[151673,151674],[151674,151675],[151675,151676]]\n",
    "lists=[[151507, 151508, 151509, 151510], [151669, 151670, 151671, 151672], [151673, 151674, 151675, 151676]]\n",
    "dirs=\"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/data/DLPFC/\"\n",
    "\n",
    "# dirs=\"/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12\"\n",
    "# sample_name=[151507,151508,151509,151510,151669,151670,151671,151672,151673,151674,151675,151676]\n",
    "for sample_name in lists:\n",
    "    # IDX=[0,1]\n",
    "    IDX=[0,1,2,3]\n",
    "    flags=str(sample_name[IDX[0]])\n",
    "    for i in np.arange(1,len(IDX)):\n",
    "        flags=flags+'-'+str(sample_name[IDX[i]])\n",
    "    for i in IDX:\n",
    "        sample1=sample_name[i]\n",
    "        features=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags+'_'+str(sample1)+\"_features-1.txt\",header=0,index_col=0,sep=',')\n",
    "        meta=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags+'_'+str(sample1)+\"_label-1.txt\",header=0,index_col=0,sep=',')\n",
    "        coord=pd.read_csv(dirs+\"gtt_input_scanpy/\"+flags+'_'+str(sample1)+\"_positions-1.txt\",header=0,index_col=0,sep=',')\n",
    "        # meta=meta.iloc[:meta.shape[0]-1,:]\n",
    "        adata = sc.AnnData(features)\n",
    "        adata.var_names_make_unique()\n",
    "        adata.X=sp.csr_matrix(adata.X)\n",
    "        adata.obsm[\"spatial\"] = coord.loc[:,['x','y']].to_numpy()\n",
    "        Cal_Spatial_Net(adata, rad_cutoff=rad, k_cutoff=6, model='KNN', verbose=True)\n",
    "        if 'highly_variable' in adata.var.columns:\n",
    "            adata_Vars =  adata[:, adata.var['highly_variable']]\n",
    "        else:\n",
    "            adata_Vars = adata\n",
    "        features = pd.DataFrame(adata_Vars.X.toarray()[:, ], index=adata_Vars.obs.index, columns=adata_Vars.var.index)\n",
    "        cells = np.array(features.index)\n",
    "        cells_id_tran = dict(zip(cells, range(cells.shape[0])))\n",
    "        if 'Spatial_Net' not in adata.uns.keys():\n",
    "            raise ValueError(\"Spatial_Net is not existed! Run Cal_Spatial_Net first!\")\n",
    "\n",
    "        Spatial_Net = adata.uns['Spatial_Net']\n",
    "        G_df = Spatial_Net.copy()\n",
    "        np.savetxt(dirs+\"gtt_input_scanpy/\"+flags+'_'+str(sample1)+\"_edge_KNN_\"+str(KNN)+\".csv\",G_df.values[:,:2],fmt='%s')\n",
    "\n",
    "        # G_df['Cell1'] = G_df['Cell1'].map(cells_id_tran)\n",
    "        # G_df['Cell2'] = G_df['Cell2'].map(cells_id_tran)\n",
    "        # adj = sp.coo_matrix((np.ones(G_df.shape[0]), (G_df['Cell1'], G_df['Cell2'])), shape=(adata.n_obs, adata.n_obs))\n",
    "        # adj+=adj.T.multiply(adj.T>adj)-adj.multiply(adj.T>adj)\n",
    "        # features=torch.FloatTensor(features.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLPFC data integration (multi-slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import umap.umap_ as umap\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from spiral.main import SPIRAL_integration\n",
    "from spiral.layers import *\n",
    "from spiral.utils import *\n",
    "from spiral.CoordAlignment import CoordAlignment\n",
    "\n",
    "# R_dirs=\"/home/tguo/tguo2/miniconda3/envs/stnet/lib/R\"\n",
    "# os.environ['R_HOME']=R_dirs\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = \"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/data/DLPFC/gtt_input_scanpy/\"\n",
    "\n",
    "sn_ = [[151507, 151508, 151509, 151510], [151510, 151669, 151670, 151671],\n",
    "      [151671, 151672, 151674, 151675]]\n",
    "for sn in sn_[:1]:\n",
    "    # for sample_name in sn:\n",
    "    sample_name = np.array(sn)\n",
    "\n",
    "    samples = sample_name[:]\n",
    "    SEP = ','\n",
    "    net_cate = '_KNN_'\n",
    "    rad = 150\n",
    "    knn = 6\n",
    "\n",
    "    N_WALKS = knn\n",
    "    WALK_LEN = 1\n",
    "    N_WALK_LEN = knn\n",
    "    NUM_NEG = knn\n",
    "\n",
    "    feat_file = []\n",
    "    edge_file = []\n",
    "    meta_file = []\n",
    "    coord_file = []\n",
    "    flags = ''\n",
    "    flags1 = str(samples[0])\n",
    "    for i in range(1, len(samples)):\n",
    "        flags1 = flags1 + '-' + str(samples[i])\n",
    "\n",
    "    for i in range(len(sample_name)):\n",
    "        feat_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_features-1.txt\")\n",
    "        # if sample_name[i] == 151676:\n",
    "        #     edge_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_edge_Radius_\" + str(rad) + \".csv\")\n",
    "        # else:\n",
    "        edge_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_edge_KNN_\" + str(knn) + \".csv\")\n",
    "        meta_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_label-1.txt\")\n",
    "        coord_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_positions-1.txt\")\n",
    "        flags = flags + '_' + str(sample_name[i])\n",
    "    N = pd.read_csv(feat_file[0], header=0, index_col=0).shape[1]\n",
    "\n",
    "    if (len(sample_name) == 2):\n",
    "        M = 1\n",
    "    else:\n",
    "        M = len(sample_name)\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=0, help='The seed of initialization.')\n",
    "    parser.add_argument('--AEdims', type=list, default=[N,[512],32], help='Dim of encoder.')\n",
    "    parser.add_argument('--AEdimsR', type=list, default=[32,[512],N], help='Dim of decoder.')\n",
    "    parser.add_argument('--GSdims', type=list, default=[512,32], help='Dim of GraphSAGE.')\n",
    "    parser.add_argument('--zdim', type=int, default=32, help='Dim of embedding.')\n",
    "    parser.add_argument('--znoise_dim', type=int, default=4, help='Dim of noise embedding.')\n",
    "    parser.add_argument('--CLdims', type=list, default=[4,[],M], help='Dim of classifier.')\n",
    "    parser.add_argument('--DIdims', type=list, default=[28,[32,16],M], help='Dim of discriminator.')\n",
    "    parser.add_argument('--beta', type=float, default=1.0, help='weight of GraphSAGE.')\n",
    "    parser.add_argument('--agg_class', type=str, default=MeanAggregator, help='Function of aggregator.')\n",
    "    parser.add_argument('--num_samples', type=str, default=knn, help='number of neighbors to sample.')\n",
    "\n",
    "    parser.add_argument('--N_WALKS', type=int, default=N_WALKS, help='number of walks of random work for postive pairs.')\n",
    "    parser.add_argument('--WALK_LEN', type=int, default=WALK_LEN, help='walk length of random work for postive pairs.')\n",
    "    parser.add_argument('--N_WALK_LEN', type=int, default=N_WALK_LEN, help='number of walks of random work for negative pairs.')\n",
    "    parser.add_argument('--NUM_NEG', type=int, default=NUM_NEG, help='number of negative pairs.')\n",
    "\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024, help='Size of batches to train.') ####512 for withon donor;1024 for across donor###\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay.')\n",
    "    parser.add_argument('--alpha1', type=float, default=N, help='Weight of decoder loss.')\n",
    "    parser.add_argument('--alpha2', type=float, default=1, help='Weight of GraphSAGE loss.')\n",
    "    parser.add_argument('--alpha3', type=float, default=1, help='Weight of classifier loss.')\n",
    "    parser.add_argument('--alpha4', type=float, default=1, help='Weight of discriminator loss.')\n",
    "    parser.add_argument('--lamda', type=float, default=1, help='Weight of GRL.')\n",
    "    parser.add_argument('--Q', type=float, default=10, help='Weight negative loss for sage losss.')\n",
    "\n",
    "    params,unknown=parser.parse_known_args()\n",
    "\n",
    "    SPII=SPIRAL_integration(params,feat_file,edge_file,meta_file)\n",
    "    SPII.train()\n",
    "    \n",
    "    SPII.model.eval()\n",
    "    all_idx=np.arange(SPII.feat.shape[0])\n",
    "    all_layer,all_mapping=layer_map(all_idx.tolist(),SPII.adj,len(SPII.params.GSdims))\n",
    "    all_rows=SPII.adj.tolil().rows[all_layer[0]]\n",
    "    all_feature=torch.Tensor(SPII.feat.iloc[all_layer[0],:].values).float().cuda()\n",
    "    all_embed,ae_out,clas_out,disc_out=SPII.model(all_feature,all_layer,all_mapping,all_rows,SPII.params.lamda,SPII.de_act,SPII.cl_act)\n",
    "    [ae_embed,gs_embed,embed]=all_embed\n",
    "    [x_bar,x]=ae_out\n",
    "    embed=embed.cpu().detach()\n",
    "    names=['GTT_'+str(i) for i in range(embed.shape[1])]\n",
    "    embed1=pd.DataFrame(np.array(embed),index=SPII.feat.index,columns=names)\n",
    "    if not os.path.exists(dirs+\"gtt_output/\"):\n",
    "        os.makedirs(dirs+\"gtt_output/\")\n",
    "        \n",
    "    embed_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_embed_\"+str(SPII.params.batch_size)+\".csv\"\n",
    "    embed1.to_csv(embed_file)\n",
    "    meta=SPII.meta.values\n",
    "\n",
    "    embed_new=torch.cat((torch.zeros((embed.shape[0],SPII.params.znoise_dim)),embed.iloc[:,SPII.params.znoise_dim:]),dim=1)\n",
    "    xbar_new=np.array(SPII.model.agc.ae.de(embed_new.cuda(),nn.Sigmoid())[1].cpu().detach())\n",
    "    xbar_new1=pd.DataFrame(xbar_new,index=SPII.feat.index,columns=SPII.feat.columns)\n",
    "    xbar_new1.to_csv(\"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/out/gtt_output/SPIRAL\"+flags+\"_correct_\"+str(SPII.params.batch_size)+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mouse Hypothalamus data integration (multi-slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = \"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/data/mhypo/gtt_input_scanpy/\"\n",
    "\n",
    "sn_ = [['-0.04', '-0.09', '-0.14', '-0.19', '-0.24']]\n",
    "c_ = [8]\n",
    "iii = 0\n",
    "for sn in sn_:\n",
    "    # for sample_name in sn:\n",
    "    sample_name = np.array(sn)\n",
    "\n",
    "    samples = sample_name[:]\n",
    "    SEP = ','\n",
    "    net_cate = '_KNN_'\n",
    "    rad = 150\n",
    "    knn = 6\n",
    "\n",
    "    N_WALKS = knn\n",
    "    WALK_LEN = 1\n",
    "    N_WALK_LEN = knn\n",
    "    NUM_NEG = knn\n",
    "\n",
    "    feat_file = []\n",
    "    edge_file = []\n",
    "    meta_file = []\n",
    "    coord_file = []\n",
    "    flags = ''\n",
    "    flags1 = str(samples[0])\n",
    "    for i in range(1, len(samples)):\n",
    "        flags1 = flags1 + '-' + str(samples[i])\n",
    "\n",
    "    for i in range(len(sample_name)):\n",
    "        feat_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_features-1.txt\")\n",
    "        # if sample_name[i] == 151676:\n",
    "        #     edge_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_edge_Radius_\" + str(rad) + \".csv\")\n",
    "        # else:\n",
    "        edge_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_edge_KNN_\" + str(knn) + \".csv\")\n",
    "        meta_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_label-1.txt\")\n",
    "        coord_file.append(dirs + flags1 + '_' + str(sample_name[i]) + \"_positions-1.txt\")\n",
    "        flags = flags + '_' + str(sample_name[i])\n",
    "    N = pd.read_csv(feat_file[0], header=0, index_col=0).shape[1]\n",
    "\n",
    "    if (len(sample_name) == 2):\n",
    "        M = 1\n",
    "    else:\n",
    "        M = len(sample_name)\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=0, help='The seed of initialization.')\n",
    "    parser.add_argument('--AEdims', type=list, default=[N,[512],32], help='Dim of encoder.')\n",
    "    parser.add_argument('--AEdimsR', type=list, default=[32,[512],N], help='Dim of decoder.')\n",
    "    parser.add_argument('--GSdims', type=list, default=[512,32], help='Dim of GraphSAGE.')\n",
    "    parser.add_argument('--zdim', type=int, default=32, help='Dim of embedding.')\n",
    "    parser.add_argument('--znoise_dim', type=int, default=4, help='Dim of noise embedding.')\n",
    "    parser.add_argument('--CLdims', type=list, default=[4,[],M], help='Dim of classifier.')\n",
    "    parser.add_argument('--DIdims', type=list, default=[28,[32,16],M], help='Dim of discriminator.')\n",
    "    parser.add_argument('--beta', type=float, default=1.0, help='weight of GraphSAGE.')\n",
    "    parser.add_argument('--agg_class', type=str, default=MeanAggregator, help='Function of aggregator.')\n",
    "    parser.add_argument('--num_samples', type=str, default=knn, help='number of neighbors to sample.')\n",
    "\n",
    "    parser.add_argument('--N_WALKS', type=int, default=N_WALKS, help='number of walks of random work for postive pairs.')\n",
    "    parser.add_argument('--WALK_LEN', type=int, default=WALK_LEN, help='walk length of random work for postive pairs.')\n",
    "    parser.add_argument('--N_WALK_LEN', type=int, default=N_WALK_LEN, help='number of walks of random work for negative pairs.')\n",
    "    parser.add_argument('--NUM_NEG', type=int, default=NUM_NEG, help='number of negative pairs.')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "    parser.add_argument('--batch_size', type=int, default=1024, help='Size of batches to train.') ####512 for withon donor;1024 for across donor###\n",
    "    parser.add_argument('--lr', type=float, default=1e-3, help='Initial learning rate.')\n",
    "    parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay.')\n",
    "    parser.add_argument('--alpha1', type=float, default=N, help='Weight of decoder loss.')\n",
    "    parser.add_argument('--alpha2', type=float, default=1, help='Weight of GraphSAGE loss.')\n",
    "    parser.add_argument('--alpha3', type=float, default=1, help='Weight of classifier loss.')\n",
    "    parser.add_argument('--alpha4', type=float, default=1, help='Weight of discriminator loss.')\n",
    "    parser.add_argument('--lamda', type=float, default=1, help='Weight of GRL.')\n",
    "    parser.add_argument('--Q', type=float, default=10, help='Weight negative loss for sage losss.')\n",
    "\n",
    "    params,unknown=parser.parse_known_args()\n",
    "\n",
    "    SPII=SPIRAL_integration(params,feat_file,edge_file,meta_file)\n",
    "\n",
    "    if not os.path.exists(dirs+\"gtt_output/SPIRAL\"+flags+\"_embed_\"+str(SPII.params.batch_size)+\".csv\"):\n",
    "        SPII.train()\n",
    "        SPII.model.eval()\n",
    "        all_idx=np.arange(SPII.feat.shape[0])\n",
    "        all_layer,all_mapping=layer_map(all_idx.tolist(),SPII.adj,len(SPII.params.GSdims))\n",
    "        all_rows=SPII.adj.tolil().rows[all_layer[0]]\n",
    "        all_feature=torch.Tensor(SPII.feat.iloc[all_layer[0],:].values).float().cuda()\n",
    "        all_embed,ae_out,clas_out,disc_out=SPII.model(all_feature,all_layer,all_mapping,all_rows,SPII.params.lamda,SPII.de_act,SPII.cl_act)\n",
    "        [ae_embed,gs_embed,embed]=all_embed\n",
    "        [x_bar,x]=ae_out\n",
    "        embed=embed.cpu().detach()\n",
    "        names=['GTT_'+str(i) for i in range(embed.shape[1])]\n",
    "        embed1=pd.DataFrame(np.array(embed),index=SPII.feat.index,columns=names)\n",
    "        if not os.path.exists(dirs+\"gtt_output/\"):\n",
    "            os.makedirs(dirs+\"gtt_output/\")\n",
    "        \n",
    "        embed_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_embed_\"+str(SPII.params.batch_size)+\".csv\"\n",
    "        embed1.to_csv(embed_file)\n",
    "        meta=SPII.meta.values\n",
    "\n",
    "        ann.obsm['spiral']=embed.numpy()\n",
    "        print(ann.obsm['spiral'])\n",
    "    else:\n",
    "        # embed_new=torch.cat((torch.zeros((embed.shape[0],SPII.params.znoise_dim)),embed.iloc[:,SPII.params.znoise_dim:]),dim=1)\n",
    "        # xbar_new=np.array(SPII.model.agc.ae.de(embed_new.cuda(),nn.Sigmoid())[1].cpu().detach())\n",
    "        # xbar_new1=pd.DataFrame(xbar_new,index=SPII.feat.index,columns=SPII.feat.columns)\n",
    "        # xbar_new1.to_csv(\"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/out/gtt_output/SPIRAL\"+flags+\"_correct_\"+str(SPII.params.batch_size)+\".csv\")\n",
    "\n",
    "        embed_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_embed_\"+str(SPII.params.batch_size)+\".csv\"\n",
    "        embed=pd.read_csv(embed_file,header=0,index_col=0,sep=',')\n",
    "        ann=anndata.AnnData(SPII.feat)\n",
    "        print(ann.X)\n",
    "    \n",
    "        ann.obsm['spiral']=embed.to_numpy()\n",
    "        print(ann.obsm['spiral'])\n",
    "\n",
    "    # embed_new=torch.cat((torch.zeros((embed.shape[0],SPII.params.znoise_dim)),embed.iloc[:,SPII.params.znoise_dim:]),dim=1)\n",
    "    # xbar_new=np.array(SPII.model.agc.ae.de(embed_new.cuda(),nn.Sigmoid())[1].cpu().detach())\n",
    "    # xbar_new1=pd.DataFrame(xbar_new,index=SPII.feat.index,columns=SPII.feat.columns)\n",
    "    # xbar_new1.to_csv(\"/home/yunfei/spatial_benchmarking/3d_recon/SPIRAL/out/gtt_output/SPIRAL\"+flags+\"_correct_\"+str(SPII.params.batch_size)+\".csv\")\n",
    "\n",
    "    # embed_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_embed_\"+str(SPII.params.batch_size)+\".csv\"\n",
    "    # embed=pd.read_csv(embed_file,header=0,index_col=0,sep=',')\n",
    "    ann=anndata.AnnData(SPII.feat)\n",
    "    print(ann.X)\n",
    "    \n",
    "    ann.obsm['spiral']=embed.to_numpy()\n",
    "    print(ann.obsm['spiral'])\n",
    "\n",
    "    if not os.path.exists(dirs+\"gtt_output/SPIRAL\"+flags+\"_mclust.csv\"):\n",
    "        n_clust=c_[iii]\n",
    "        iii += 1\n",
    "        # res1=0.5 ####adjust to make sure 7 clusters\n",
    "        # res2=0.5\n",
    "        # sc.tl.leiden(ann,resolution=res1)\n",
    "        # sc.tl.louvain(ann,resolution=res2)\n",
    "        ann = mclust_R(ann, used_obsm='spiral', num_cluster=n_clust)\n",
    "        sc.pp.neighbors(ann,use_rep='spiral')\n",
    "        \n",
    "        ann.obs['batch']=SPII.meta.loc[:,'batch'].values\n",
    "        ub=np.unique(ann.obs['batch'])\n",
    "        sc.tl.umap(ann)\n",
    "        coord=pd.read_csv(coord_file[0],header=0,index_col=0)\n",
    "        for i in np.arange(1,len(samples)):\n",
    "            coord=pd.concat((coord,pd.read_csv(coord_file[i],header=0,index_col=0)))\n",
    "\n",
    "        coord.columns=['y','x']\n",
    "        ann.obsm['spatial']=coord.loc[ann.obs_names,:].values\n",
    "        cluster_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_mclust.csv\"\n",
    "        pd.DataFrame(ann.obs['mclust']).to_csv(cluster_file)\n",
    "    else:\n",
    "        cluster_file=dirs+\"gtt_output/SPIRAL\"+flags+\"_mclust.csv\"\n",
    "    clust_cate='louvain'\n",
    "    input_file=[meta_file,coord_file,embed_file,cluster_file]\n",
    "    output_dirs=dirs+\"gtt_output/SPIRAL_alignment/\"\n",
    "    if not os.path.exists(output_dirs):\n",
    "        os.makedirs(output_dirs)\n",
    "    ub=samples\n",
    "\n",
    "    alpha=0.5\n",
    "    types=\"weighted_mean\"\n",
    "    R_dirs=\"/home/yunfei/anaconda3/envs/spiral/lib/R\"\n",
    "    CA=CoordAlignment(input_file=input_file,output_dirs=output_dirs,ub=ub,flags=flags,clust_cate=clust_cate,R_dirs=R_dirs,alpha=alpha,types=types)\n",
    "    New_Coord=CA.New_Coord\n",
    "    New_Coord.to_csv(output_dirs+\"new_coord\"+flags+\"_modify.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
