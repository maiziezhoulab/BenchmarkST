{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAligner integration tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import STAligner\n",
    "\n",
    "# the location of R (used for the mclust clustering)\n",
    "import os\n",
    "import rpy2.robjects as robjects\n",
    "import rpy2.robjects.numpy2ri\n",
    "\n",
    "from st_loading_utils import load_mHypothalamus, load_DLPFC, load_spacelhBC\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import scipy.linalg\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "import time\n",
    "import torch\n",
    "used_device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "iters = 1\n",
    "\n",
    "save_dir_gt = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mouse Hypothalamus data integration (pair-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"integration mhypo\"\"\"\n",
    "\n",
    "section_ids_list = [['-0.04', '-0.09'], ['-0.09', '-0.14'], ['-0.14', '-0.19'], ['-0.19', '-0.24']]\n",
    "run_times = []\n",
    "for iter_ in range(iters):\n",
    "    for section_ids in section_ids_list:\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        dataset = section_ids[0] + '_' + section_ids[1]\n",
    "        print(dataset)\n",
    "        start_time = time.time()\n",
    "        for section_id in section_ids:\n",
    "            adata = load_mHypothalamus(root_dir='/home/yunfei/spatial_benchmarking/benchmarking_data/mHypothalamus', section_id=section_id)\n",
    "            adata.var_names_make_unique(join=\"++\")\n",
    "            \n",
    "            # make spot name unique\n",
    "            adata.obs_names = [x+'_'+section_id for x in adata.obs_names]\n",
    "\n",
    "            adata.X = sp.csr_matrix(adata.X)\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            STAligner.Cal_Spatial_Net(adata, rad_cutoff=35) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            # STAligner.Stats_Spatial_Net(adata) # plot the number of spatial neighbors\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(adata.uns['adj'])\n",
    "            Batch_list.append(adata)\n",
    "\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids)\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "        print('adata_concat.shape: ', adata_concat.shape)\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        adata_concat.uns['edgeList'] = np.nonzero(adj_concat)\n",
    "\n",
    "        \n",
    "        adata_concat = STAligner.train_STAligner(adata_concat, verbose=True, knn_neigh = 50, device=used_device)\n",
    "        STAligner.mclust_R(adata_concat, num_cluster=7, used_obsm='STAligner')\n",
    "        adata_concat = adata_concat[adata_concat.obs['original_clusters']!='unknown']\n",
    "        end_time = time.time()\n",
    "        run_times.append(end_time - start_time)\n",
    "        \n",
    "        print('mclust, ARI = %01.3f' % ari_score(adata_concat.obs['original_clusters'], adata_concat.obs['mclust']))\n",
    "        if not os.path.exists(os.path.join(save_dir_gt, dataset)):\n",
    "            os.makedirs(os.path.join(save_dir_gt, dataset))\n",
    "        # embedding in adata.obsm:key='STAligner'\n",
    "        # print(adata_concat.obs)\n",
    "        # print(adata_concat.obsm['STAligner'])\n",
    "        df_labels = adata_concat.obs[['original_clusters', 'mclust']]\n",
    "        embed_ = adata_concat.obsm['STAligner']\n",
    "        # adata_concat.write_h5ad(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'.h5ad'))\n",
    "        np.save(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'embedding.npy'), embed_)\n",
    "        df_labels.to_csv(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'labels.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLPFC data integration (pair-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"integration dlpfc\"\"\"\n",
    "\n",
    "section_ids_list = [['151507', '151508'], ['151508', '151509'], ['151509', '151510'], ['151673', '151674'], ['151674', '151675'], ['151675', '151676']]\n",
    "\n",
    "run_times = []\n",
    "for iter_ in range(iters):\n",
    "    for section_ids in section_ids_list:\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        dataset = section_ids[0] + '_' + section_ids[1]\n",
    "        print(dataset)\n",
    "        start_time = time.time()\n",
    "        for section_id in section_ids:\n",
    "            adata = load_DLPFC(root_dir='/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12/', section_id=section_id)\n",
    "            adata.var_names_make_unique(join=\"++\")\n",
    "            \n",
    "            # make spot name unique\n",
    "            adata.obs_names = [x+'_'+section_id for x in adata.obs_names]\n",
    "\n",
    "            adata.X = sp.csr_matrix(adata.X)\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            STAligner.Cal_Spatial_Net(adata, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            # STAligner.Stats_Spatial_Net(adata) # plot the number of spatial neighbors\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(adata.uns['adj'])\n",
    "            Batch_list.append(adata)\n",
    "\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids)\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "        print('adata_concat.shape: ', adata_concat.shape)\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        adata_concat.uns['edgeList'] = np.nonzero(adj_concat)\n",
    "\n",
    "        \n",
    "        adata_concat = STAligner.train_STAligner(adata_concat, verbose=True, knn_neigh = 50, device=used_device)\n",
    "        STAligner.mclust_R(adata_concat, num_cluster=7, used_obsm='STAligner')\n",
    "        adata_concat = adata_concat[adata_concat.obs['original_clusters']!='unknown']\n",
    "        end_time = time.time()\n",
    "        run_times.append(end_time - start_time)\n",
    "\n",
    "        print('mclust, ARI = %01.3f' % ari_score(adata_concat.obs['original_clusters'], adata_concat.obs['mclust']))\n",
    "        if not os.path.exists(os.path.join(save_dir_gt, dataset)):\n",
    "            os.makedirs(os.path.join(save_dir_gt, dataset))\n",
    "        # embedding in adata.obsm:key='STAligner'\n",
    "        # print(adata_concat.obs)\n",
    "        # print(adata_concat.obsm['STAligner'])\n",
    "        df_labels = adata_concat.obs[['original_clusters', 'mclust']]\n",
    "        embed_ = adata_concat.obsm['STAligner']\n",
    "        # adata_concat.write_h5ad(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'.h5ad'))\n",
    "        np.save(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'embedding.npy'), embed_)\n",
    "        df_labels.to_csv(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'labels.csv'))\n",
    " \n",
    "section_ids_list = [['151669', '151670'], ['151670', '151671'], ['151671', '151672']]\n",
    "\n",
    "run_times = []\n",
    "for iter_ in range(iters):\n",
    "    for section_ids in section_ids_list:\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        dataset = section_ids[0] + '_' + section_ids[1]\n",
    "        print(dataset)\n",
    "        start_time = time.time()\n",
    "        for section_id in section_ids:\n",
    "            adata = load_DLPFC(root_dir='/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12/', section_id=section_id)\n",
    "            adata.var_names_make_unique(join=\"++\")\n",
    "            \n",
    "            # make spot name unique\n",
    "            adata.obs_names = [x+'_'+section_id for x in adata.obs_names]\n",
    "\n",
    "            adata.X = sp.csr_matrix(adata.X)\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            STAligner.Cal_Spatial_Net(adata, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            # STAligner.Stats_Spatial_Net(adata) # plot the number of spatial neighbors\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(adata.uns['adj'])\n",
    "            Batch_list.append(adata)\n",
    "\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids)\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "        print('adata_concat.shape: ', adata_concat.shape)\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        adata_concat.uns['edgeList'] = np.nonzero(adj_concat)\n",
    "\n",
    "        \n",
    "        adata_concat = STAligner.train_STAligner(adata_concat, verbose=True, knn_neigh = 50, device=used_device)\n",
    "        STAligner.mclust_R(adata_concat, num_cluster=7, used_obsm='STAligner')\n",
    "        adata_concat = adata_concat[adata_concat.obs['original_clusters']!='unknown']\n",
    "        end_time = time.time()\n",
    "        run_times.append(end_time - start_time)\n",
    "\n",
    "        print('mclust, ARI = %01.3f' % ari_score(adata_concat.obs['original_clusters'], adata_concat.obs['mclust']))\n",
    "        if not os.path.exists(os.path.join(save_dir_gt, dataset)):\n",
    "            os.makedirs(os.path.join(save_dir_gt, dataset))\n",
    "        # embedding in adata.obsm:key='STAligner'\n",
    "        # print(adata_concat.obs)\n",
    "        # print(adata_concat.obsm['STAligner'])\n",
    "        df_labels = adata_concat.obs[['original_clusters', 'mclust']]\n",
    "        embed_ = adata_concat.obsm['STAligner']\n",
    "        # adata_concat.write_h5ad(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'.h5ad'))\n",
    "        np.save(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'embedding.npy'), embed_)\n",
    "        df_labels.to_csv(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mouse Hypothalamus data integration (multi-slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_ids_list = [['-0.04', '-0.09', '-0.14', '-0.19', '-0.24']]\n",
    "run_times = []\n",
    "for iter_ in range(iters):\n",
    "    for section_ids in section_ids_list:\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        dataset = section_ids[0] + '_' + section_ids[1] + '_' + section_ids[2] + '_' + section_ids[3] + '_' + section_ids[4]\n",
    "        print(dataset)\n",
    "        start_time = time.time()\n",
    "        for section_id in section_ids:\n",
    "            adata = load_mHypothalamus(root_dir='/home/yunfei/spatial_benchmarking/benchmarking_data/mHypothalamus', section_id=section_id)\n",
    "            adata.var_names_make_unique(join=\"++\")\n",
    "            \n",
    "            # make spot name unique\n",
    "            adata.obs_names = [x+'_'+section_id for x in adata.obs_names]\n",
    "\n",
    "            adata.X = sp.csr_matrix(adata.X)\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            STAligner.Cal_Spatial_Net(adata, rad_cutoff=35) # the spatial network are saved in adata.uns[‘adj’d]\n",
    "            # STAligner.Stats_Spatial_Net(adata) # plot the number of spatial neighbors\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(adata.uns['adj'])\n",
    "            Batch_list.append(adata)\n",
    "\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids)\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "        print('adata_concat.shape: ', adata_concat.shape)\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        adata_concat.uns['edgeList'] = np.nonzero(adj_concat)\n",
    "\n",
    "        \n",
    "        adata_concat = STAligner.train_STAligner(adata_concat, verbose=True, knn_neigh = 50, device=used_device)\n",
    "        STAligner.mclust_R(adata_concat, num_cluster=7, used_obsm='STAligner')\n",
    "        adata_concat = adata_concat[adata_concat.obs['original_clusters']!='unknown']\n",
    "        end_time = time.time()\n",
    "        run_times.append(end_time - start_time)\n",
    "        \n",
    "        print('mclust, ARI = %01.3f' % ari_score(adata_concat.obs['original_clusters'], adata_concat.obs['mclust']))\n",
    "        if not os.path.exists(os.path.join(save_dir_gt, dataset)):\n",
    "            os.makedirs(os.path.join(save_dir_gt, dataset))\n",
    "        # embedding in adata.obsm:key='STAligner'\n",
    "        # print(adata_concat.obs)\n",
    "        # print(adata_concat.obsm['STAligner'])\n",
    "        df_labels = adata_concat.obs[['original_clusters', 'mclust']]\n",
    "        embed_ = adata_concat.obsm['STAligner']\n",
    "        # adata_concat.write_h5ad(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'.h5ad'))\n",
    "        np.save(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'embedding.npy'), embed_)\n",
    "        df_labels.to_csv(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'labels.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLPFC data integration (multi-slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_ids_list = [['151507', '151508', '151509', '151510']]\n",
    "run_times = []\n",
    "for iter_ in range(iters):\n",
    "    for section_ids in section_ids_list:\n",
    "        Batch_list = []\n",
    "        adj_list = []\n",
    "        dataset = section_ids[0] + '_' + section_ids[1] + '_' + section_ids[2] + '_' + section_ids[3]\n",
    "        print(dataset)\n",
    "        start_time = time.time()\n",
    "        for section_id in section_ids:\n",
    "            adata = load_DLPFC(root_dir='/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12/', section_id=section_id)\n",
    "            adata.var_names_make_unique(join=\"++\")\n",
    "            \n",
    "            # make spot name unique\n",
    "            adata.obs_names = [x+'_'+section_id for x in adata.obs_names]\n",
    "\n",
    "            adata.X = sp.csr_matrix(adata.X)\n",
    "            \n",
    "            # Constructing the spatial network\n",
    "            STAligner.Cal_Spatial_Net(adata, rad_cutoff=150) # the spatial network are saved in adata.uns[‘adj’]\n",
    "            # STAligner.Stats_Spatial_Net(adata) # plot the number of spatial neighbors\n",
    "            \n",
    "            # Normalization\n",
    "            sc.pp.highly_variable_genes(adata, flavor=\"seurat_v3\", n_top_genes=5000)\n",
    "            sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "            sc.pp.log1p(adata)\n",
    "            adata = adata[:, adata.var['highly_variable']]\n",
    "\n",
    "            adj_list.append(adata.uns['adj'])\n",
    "            Batch_list.append(adata)\n",
    "\n",
    "        adata_concat = anndata.concat(Batch_list, label=\"slice_name\", keys=section_ids)\n",
    "        adata_concat.obs['original_clusters'] = adata_concat.obs['original_clusters'].astype('category')\n",
    "        adata_concat.obs[\"batch_name\"] = adata_concat.obs[\"slice_name\"].astype('category')\n",
    "        print('adata_concat.shape: ', adata_concat.shape)\n",
    "\n",
    "        adj_concat = np.asarray(adj_list[0].todense())\n",
    "        for batch_id in range(1,len(section_ids)):\n",
    "            adj_concat = scipy.linalg.block_diag(adj_concat, np.asarray(adj_list[batch_id].todense()))\n",
    "        adata_concat.uns['edgeList'] = np.nonzero(adj_concat)\n",
    "\n",
    "        \n",
    "        adata_concat = STAligner.train_STAligner(adata_concat, verbose=True, knn_neigh = 50, device=used_device)\n",
    "        STAligner.mclust_R(adata_concat, num_cluster=7, used_obsm='STAligner')\n",
    "        adata_concat = adata_concat[adata_concat.obs['original_clusters']!='unknown']\n",
    "        end_time = time.time()\n",
    "        run_times.append(end_time - start_time)\n",
    "\n",
    "        print('mclust, ARI = %01.3f' % ari_score(adata_concat.obs['original_clusters'], adata_concat.obs['mclust']))\n",
    "        if not os.path.exists(os.path.join(save_dir_gt, dataset)):\n",
    "            os.makedirs(os.path.join(save_dir_gt, dataset))\n",
    "        # embedding in adata.obsm:key='STAligner'\n",
    "        # print(adata_concat.obs)\n",
    "        # print(adata_concat.obsm['STAligner'])\n",
    "        df_labels = adata_concat.obs[['original_clusters', 'mclust']]\n",
    "        embed_ = adata_concat.obsm['STAligner']\n",
    "        # adata_concat.write_h5ad(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'.h5ad'))\n",
    "        np.save(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'embedding.npy'), embed_)\n",
    "        df_labels.to_csv(os.path.join(save_dir_gt, dataset, 'iter'+str(iter_)+'labels.csv'))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
