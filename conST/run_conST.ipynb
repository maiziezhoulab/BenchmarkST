{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import packages and select GPU if accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.graph_func import graph_construction\n",
    "from src.utils_func import mk_dir, adata_preprocess, load_ST_file, res_search_fixed_clus, plot_clustering\n",
    "from src.training import conST_training\n",
    "\n",
    "import anndata\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from st_loading_utils import load_DLPFC, load_BC, load_mVC, load_mPFC, load_mHypothalamus, load_her2_tumor, load_mMAMP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--k', type=int, default=10, help='parameter k in spatial graph')\n",
    "parser.add_argument('--knn_distanceType', type=str, default='euclidean',\n",
    "                    help='graph distance type: euclidean/cosine/correlation')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "parser.add_argument('--cell_feat_dim', type=int, default=300, help='Dim of PCA')\n",
    "parser.add_argument('--feat_hidden1', type=int, default=100, help='Dim of DNN hidden 1-layer.')\n",
    "parser.add_argument('--feat_hidden2', type=int, default=20, help='Dim of DNN hidden 2-layer.')\n",
    "parser.add_argument('--gcn_hidden1', type=int, default=32, help='Dim of GCN hidden 1-layer.')\n",
    "parser.add_argument('--gcn_hidden2', type=int, default=8, help='Dim of GCN hidden 2-layer.')\n",
    "parser.add_argument('--p_drop', type=float, default=0.2, help='Dropout rate.')\n",
    "parser.add_argument('--use_img', type=bool, default=False, help='Use histology images.')\n",
    "parser.add_argument('--img_w', type=float, default=0.1, help='Weight of image features.')\n",
    "parser.add_argument('--use_pretrained', type=bool, default=True, help='Use pretrained weights.')\n",
    "parser.add_argument('--using_mask', type=bool, default=False, help='Using mask for multi-dataset.')\n",
    "parser.add_argument('--feat_w', type=float, default=10, help='Weight of DNN loss.')\n",
    "parser.add_argument('--gcn_w', type=float, default=0.1, help='Weight of GCN loss.')\n",
    "parser.add_argument('--dec_kl_w', type=float, default=10, help='Weight of DEC loss.')\n",
    "parser.add_argument('--gcn_lr', type=float, default=0.01, help='Initial GNN learning rate.')\n",
    "parser.add_argument('--gcn_decay', type=float, default=0.01, help='Initial decay rate.')\n",
    "parser.add_argument('--dec_cluster_n', type=int, default=10, help='DEC cluster number.')\n",
    "parser.add_argument('--dec_interval', type=int, default=20, help='DEC interval nnumber.')\n",
    "parser.add_argument('--dec_tol', type=float, default=0.00, help='DEC tol.')\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
    "parser.add_argument('--beta', type=float, default=100, help='beta value for l2c')\n",
    "parser.add_argument('--cont_l2l', type=float, default=0.3, help='Weight of local contrastive learning loss.')\n",
    "parser.add_argument('--cont_l2c', type=float, default= 0.1, help='Weight of context contrastive learning loss.')\n",
    "parser.add_argument('--cont_l2g', type=float, default= 0.1, help='Weight of global contrastive learning loss.')\n",
    "\n",
    "parser.add_argument('--edge_drop_p1', type=float, default=0.1, help='drop rate of adjacent matrix of the first view')\n",
    "parser.add_argument('--edge_drop_p2', type=float, default=0.1, help='drop rate of adjacent matrix of the second view')\n",
    "parser.add_argument('--node_drop_p1', type=float, default=0.2, help='drop rate of node features of the first view')\n",
    "parser.add_argument('--node_drop_p2', type=float, default=0.3, help='drop rate of node features of the second view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________ Eval clustering Setting ______________\n",
    "parser.add_argument('--eval_resolution', type=int, default=1, help='Eval cluster number.')\n",
    "parser.add_argument('--eval_graph_n', type=int, default=20, help='Eval graph kN tol.') \n",
    "\n",
    "params =  parser.parse_args(args=['--k', '20', '--knn_distanceType', 'euclidean', '--epochs', '200'])\n",
    "\n",
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device: ' + device)\n",
    "params.device = device\n",
    "\n",
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "iters=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DLPFC dataset (12 slides)\n",
    "\n",
    "change '${dir_}' to  'path/to/your/DLPFC/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DLPFC\"\"\"\n",
    "setting_combinations = [[7, '151507'], [7, '151508'], [7, '151509'], [7, '151510'], [5, '151669'], [5, '151670'], [5, '151671'], [5, '151672'], [7, '151673'], [7, '151674'], [7, '151675'], [7, '151676']]\n",
    "for setting_combi in setting_combinations:\n",
    "    path = './benchmarking_data/DLPFC12'\n",
    "    adata_h5 = load_DLPFC(root_dir=path, section_id=setting_combi[1])\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "    \n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/spatialLIBD/'\n",
    "    # data_root = '../spatialLIBD'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # np.save(f'{params.save_path}/conST_result.npy', conST_embedding)\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('DLPFC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BC/MA datasets (2 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BC\"\"\"\n",
    "setting_combinations = [[20, 'section1']]\n",
    "for setting_combi in setting_combinations:\n",
    "    path = './benchmarking_data/BC'\n",
    "    adata_h5 = load_DLPFC(root_dir=path, section_id=setting_combi[1])\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/BC/'\n",
    "    # data_root = '../BC'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # np.save(f'{params.save_path}/conST_result.npy', conST_embedding)\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('BC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MA\"\"\"\n",
    "setting_combinations = [[52, 'MA']]\n",
    "for setting_combi in setting_combinations:\n",
    "    path = './benchmarking_data/mMAMP'\n",
    "    adata_h5 = load_mMAMP(root_dir=path, section_id=setting_combi[1])\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/MA/'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('mAB ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. mVC/mPFC datasets (4 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mVC\"\"\"\n",
    "setting_combinations = [[7, 'STARmap_20180505_BY3_1k.h5ad']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = './benchmarking_data/STARmap_mouse_visual_cortex'\n",
    "    adata_h5 = load_mVC(root_dir=path, section_id=setting_combi[1])\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/her2tumor/'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        # adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('mVC ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mPFC\"\"\"\n",
    "setting_combinations = [[4, '20180417_BZ5_control'], [4, '20180419_BZ9_control'], [4, '20180424_BZ14_control']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "    path = args.path = './benchmarking_data/STARmap_mouse_PFC'\n",
    "    adata_h5 = load_mPFC(root_dir=path, section_id=setting_combi[1])\n",
    "    if params.cell_feat_dim > len(adata_h5.var.index):\n",
    "        params.cell_feat_dim = len(adata_h5.var.index)-1\n",
    "        print(params.cell_feat_dim)\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/her2tumor/'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        # adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('mPFC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. mHypothalamus dataset (6 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mHypo\"\"\"\n",
    "setting_combinations = [[8, '-0.04'], [8, '-0.09'], [8, '-0.14'], [8, '-0.19'], [8, '-0.24'], [8, '-0.29']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "    path = args.path = './benchmarking_data/mHypothalamus'\n",
    "    adata_h5 = load_mHypothalamus(root_dir=path, section_id=setting_combi[1])\n",
    "    if params.cell_feat_dim > len(adata_h5.var.index):\n",
    "        params.cell_feat_dim = len(adata_h5.var.index)-1\n",
    "        # print(params.cell_feat_dim)\n",
    "    if params.cell_feat_dim > len(adata_h5.obs.index):\n",
    "        params.cell_feat_dim = len(adata_h5.obs.index)-1\n",
    "        # print(params.cell_feat_dim)\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/her2tumor/'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        # adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('mHypothalamus' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Her2Tumor dataset (8 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Her2st\"\"\"\n",
    "setting_combinations = [[5, 'B1'], [4, 'C1'], [4, 'D1'], [4, 'E1'], [4, 'F1'], [7, 'G2'], [7, 'H1']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args = parser.parse_args()\n",
    "    # seed\n",
    "    seed_torch(1)\n",
    "\n",
    "    path = args.path = './benchmarking_data/Her2_tumor'\n",
    "    adata_h5 = load_her2_tumor(root_dir=path, section_id=setting_combi[1])\n",
    "    if params.cell_feat_dim > len(adata_h5.obs.index):\n",
    "        params.cell_feat_dim = len(adata_h5.obs.index)-1\n",
    "        print(params.cell_feat_dim)\n",
    "    adata_X = adata_preprocess(adata_h5, min_cells=5, pca_n_comps=params.cell_feat_dim)\n",
    "    graph_dict = graph_construction(adata_h5.obsm['spatial'], adata_h5.shape[0], params)\n",
    "\n",
    "    dataset = data_name = setting_combi[1]\n",
    "    n_clusters = setting_combi[0]\n",
    "    aris = []\n",
    "    save_root = './output/her2tumor/'\n",
    "    # data_root = '../BC'\n",
    "    params.save_path = mk_dir(f'{save_root}/{data_name}/conST')\n",
    "\n",
    "    params.cell_num = adata_h5.shape[0]\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        seed_torch(params.seed)\n",
    "        \n",
    "        if params.use_img:\n",
    "            img_transformed = np.load('./MAE-pytorch/extracted_feature.npy')\n",
    "            img_transformed = (img_transformed - img_transformed.mean()) / img_transformed.std() * adata_X.std() + adata_X.mean()\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters, img_transformed)\n",
    "        else:\n",
    "            conST_net = conST_training(adata_X, graph_dict, params, n_clusters)\n",
    "\n",
    "        conST_net.pretraining()\n",
    "        conST_net.major_training()\n",
    "\n",
    "        conST_embedding = conST_net.get_embedding()\n",
    "\n",
    "        # clustering\n",
    "        adata_conST = anndata.AnnData(conST_embedding, obs=adata_h5.obs)\n",
    "        # adata_conST.uns['spatial'] = adata_h5.uns['spatial']\n",
    "        adata_conST.obs['original_clusters'] = adata_h5.obs['original_clusters']\n",
    "        adata_conST.obsm['spatial'] = adata_h5.obsm['spatial']\n",
    "\n",
    "        sc.pp.neighbors(adata_conST, n_neighbors=params.eval_graph_n)\n",
    "\n",
    "        eval_resolution = res_search_fixed_clus(adata_conST, n_clusters)\n",
    "        print(eval_resolution)\n",
    "        cluster_key = \"conST_leiden\"\n",
    "        sc.tl.leiden(adata_conST, key_added=cluster_key, resolution=eval_resolution)\n",
    "\n",
    "        keep_bcs = adata_conST.obs.dropna().index\n",
    "        adata_conST = adata_conST[keep_bcs].copy()\n",
    "        ARI = metrics.adjusted_rand_score(adata_conST.obs[cluster_key], adata_conST.obs['original_clusters'])\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('const_aris.txt', 'a+') as fp:\n",
    "        fp.write('Her2tumor' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
