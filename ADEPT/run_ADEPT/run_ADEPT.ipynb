{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. import packages and select GPU if accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import os\n",
    "from imputation.impute import impute_\n",
    "import GAAE\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42\n",
    "import time\n",
    "import seaborn as sns \n",
    "from GAAE.utils import impute, DE_num_calc, initialize, filter_num_calc, downstream_analyses \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_dir', type=str, default=\"./\", help=\"root dir for input data\")\n",
    "parser.add_argument('--gt_dir', type=str, default=\"./\", help=\"root dir for data ground truth\")\n",
    "parser.add_argument('--input_data', type=str, default=\"151673\", help=\"input data section id\")\n",
    "parser.add_argument('--impute_cluster_num', type=str, default=\"7\", help=\"diff cluster numbers for imputation\")\n",
    "parser.add_argument('--cluster_num', type=int, default=7, help=\"input data cluster number\")\n",
    "parser.add_argument('--radius', type=int, default=150, help=\"input data radius\")\n",
    "parser.add_argument(\"--de_candidates\", type=str, default=\"200\", help=\"candidate de list for imputation, separated by comma\")\n",
    "parser.add_argument('--no_de', type=int, default=0, help='switch on/off DEG selection module')\n",
    "parser.add_argument(\"--use_mean\", type=int, default=0, help=\"use mean value in de list or not\")\n",
    "parser.add_argument(\"--impute_runs\", type=int, default=2, help=\"time of runs for imputation\")\n",
    "parser.add_argument(\"--runs\", type=int, default=20, help=\"total runs for the data\")\n",
    "parser.add_argument('--gt', type=int, default=1, help=\"ground truth for the input data\")\n",
    "parser.add_argument('--use_hvgs', type=int, default=3000, help=\"select highly variable genes before training\")\n",
    "parser.add_argument('--use_preprocessing', type=int, default=1, help='use preprocessed input or raw input')\n",
    "parser.add_argument('--save_fig', type=int, default=1, help='saving output visualization')\n",
    "parser.add_argument('--filter_nzr', type=float, default=0.15, help='suggested nzr threshold for filtering')\n",
    "parser.add_argument('--filter_num', type=int, default=None, help='suggested gene threshold for filtering')\n",
    "parser.add_argument('--de_nzr_min', type=float, default=0.299, help='suggested min nzr threshold after de selection')\n",
    "parser.add_argument('--de_nzr_max', type=float, default=0.399, help='suggested max nzr threshold after de selection')\n",
    "parser.add_argument('--use_gpu_id', type=str, default='1', help='use which GPU, only applies when you have multiple gpu')\n",
    "args = parser.parse_args()\n",
    "args.impute_cluster_num = args.impute_cluster_num.split(\",\")  # [\"5\", \"6\", \"7\"]\n",
    "\n",
    "\n",
    "# device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using device: ' + args.use_gpu_id)\n",
    "# params.device = device\n",
    "\n",
    "\n",
    "iters=20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. DLPFC dataset (12 slides)\n",
    "\n",
    "change '${dir_}' to  'path/to/your/DLPFC/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DLPFC\"\"\"\n",
    "# the number of clusters\n",
    "setting_combinations = [[7, '151507'], [7, '151508'], [7, '151509'], [7, '151510'], [5, '151669'], [5, '151670'], [5, '151671'], [5, '151672'], [7, '151673'], [7, '151674'], [7, '151675'], [7, '151676']]\n",
    "# setting_combinations = [[7, '151674'], [7, '151675'], [7, '151676']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/DLPFC12'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 150\n",
    "    args.use_preprocessing = 1\n",
    "    args.use_hvgs = 0\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/DLPFC' + dataset + '.txt'):\n",
    "            with open('./cache/DLPFC' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/DLPFC' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    \n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        adata_list = []\n",
    "        for de_ in de_top_k_list:\n",
    "            for cluster_n in args.impute_cluster_num:\n",
    "                print(\"cluster_n = \", cluster_n)\n",
    "                GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                               num_cluster=int(cluster_n),\n",
    "                                                                               dif_k=de_, device_id=args.use_gpu_id)\n",
    "                de_list_epoch.append(de_list)\n",
    "                adata_list.append(adata_out)\n",
    "        g_union = set.union(*de_list_epoch)\n",
    "        imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "        ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('DLPFC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. BC/MA datasets (2 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BC\"\"\"\n",
    "# the number of clusters\n",
    "setting_combinations = [[20, 'section1']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/BC'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 450\n",
    "    args.use_preprocessing = 1\n",
    "    args.use_hvgs = 0\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/BC' + dataset + '.txt'):\n",
    "            with open('./cache/BC' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/BC' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    \n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        adata_list = []\n",
    "        if de_top_k_list != []:\n",
    "            print(\"performing DEGs selection\")\n",
    "            for de_ in de_top_k_list:\n",
    "                for cluster_n in args.impute_cluster_num:\n",
    "                    print(\"cluster_n = \", cluster_n)\n",
    "                    GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                    ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                                num_cluster=int(cluster_n),\n",
    "                                                                                dif_k=de_, device_id=args.use_gpu_id)\n",
    "                    de_list_epoch.append(de_list)\n",
    "                    adata_list.append(adata_out)\n",
    "            g_union = set.union(*de_list_epoch)\n",
    "            imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "        else:\n",
    "            print(\"skip performing DEGs selection\")\n",
    "            imputed_ad = adata\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        if de_top_k_list != []:\n",
    "            GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "            ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "        else:\n",
    "            GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "            ARI, adata_out = GAAE.train_ADEPT(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('BC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MA\"\"\"\n",
    "setting_combinations = [[52, 'MA']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/mMAMP'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 150\n",
    "    args.use_preprocessing = 1\n",
    "    args.use_hvgs = 0\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/MA' + dataset + '.txt'):\n",
    "            with open('./cache/MA' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/DLPFC' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    \n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        adata_list = []\n",
    "        if de_top_k_list != []:\n",
    "            print(\"performing DEGs selection\")\n",
    "            for de_ in de_top_k_list:\n",
    "                for cluster_n in args.impute_cluster_num:\n",
    "                    print(\"cluster_n = \", cluster_n)\n",
    "                    GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                    ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                                num_cluster=int(cluster_n),\n",
    "                                                                                dif_k=de_, device_id=args.use_gpu_id)\n",
    "                    de_list_epoch.append(de_list)\n",
    "                    adata_list.append(adata_out)\n",
    "            g_union = set.union(*de_list_epoch)\n",
    "            imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "        else:\n",
    "            print(\"skip performing DEGs selection\")\n",
    "            imputed_ad = adata\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        if de_top_k_list != []:\n",
    "            GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "            ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "        else:\n",
    "            GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "            ARI, adata_out = GAAE.train_ADEPT(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('mAB' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. mVC/mPFC datasets (4 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mVC\"\"\"\n",
    "setting_combinations = [[7, 'STARmap_20180505_BY3_1k.h5ad']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/STARmap_mouse_visual_cortex'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 450\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/DLPFC' + dataset + '.txt'):\n",
    "            with open('./cache/DLPFC' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/DLPFC' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    adata_list = []\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        for de_ in de_top_k_list:\n",
    "            for cluster_n in args.impute_cluster_num:\n",
    "                print(\"cluster_n = \", cluster_n)\n",
    "                GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                               num_cluster=int(cluster_n),\n",
    "                                                                               dif_k=de_, device_id=args.use_gpu_id)\n",
    "                de_list_epoch.append(de_list)\n",
    "                adata_list.append(adata_out)\n",
    "        g_union = set.union(*de_list_epoch)\n",
    "        imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "        ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('mVC ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mPFC\"\"\"\n",
    "# the number of clusters [4, '20180417_BZ5_control'], [4, '20180419_BZ9_control'], \n",
    "setting_combinations = [[4, '20180424_BZ14_control']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/STARmap_mouse_PFC'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 450\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/mPFC' + dataset + '.txt'):\n",
    "            with open('./cache/mPFC' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/mPFC' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    adata_list = []\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        for de_ in de_top_k_list:\n",
    "            for cluster_n in args.impute_cluster_num:\n",
    "                print(\"cluster_n = \", cluster_n)\n",
    "                GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                               num_cluster=int(cluster_n),\n",
    "                                                                               dif_k=de_, device_id=args.use_gpu_id)\n",
    "                de_list_epoch.append(de_list)\n",
    "                adata_list.append(adata_out)\n",
    "        g_union = set.union(*de_list_epoch)\n",
    "        imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "        ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('mPFC' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. mHypothalamus dataset (6 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mHypo\"\"\"\n",
    "setting_combinations = [[8, '-0.04'], [8, '-0.09'], [8, '-0.14'], [8, '-0.19'], [8, '-0.24'], [8, '-0.29']]\n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/mHypothalamus'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 150\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/mHypo' + dataset + '.txt'):\n",
    "            with open('./cache/mHypo' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/mHypo' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    adata_list = []\n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        if de_top_k_list != []:\n",
    "            print(\"performing DEGs selection\")\n",
    "            adata_list = []\n",
    "            for de_ in de_top_k_list:\n",
    "                for cluster_n in args.impute_cluster_num:\n",
    "                    print(\"cluster_n = \", cluster_n)\n",
    "                    GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                    ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                                num_cluster=int(cluster_n),\n",
    "                                                                                dif_k=de_, device_id=args.use_gpu_id)\n",
    "                    de_list_epoch.append(de_list)\n",
    "                    adata_list.append(adata_out)\n",
    "            g_union = set.union(*de_list_epoch)\n",
    "            imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "        else:\n",
    "            print(\"skip performing DEGs selection\")\n",
    "            imputed_ad = adata\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "        ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    print(aris)\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('mHypothalamus' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Her2Tumor dataset (8 slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Her2st\"\"\"\n",
    "setting_combinations = [[6, 'A1'], [5, 'B1'], [4, 'C1'], [4, 'D1'], [4, 'E1'], [4, 'F1'], [7, 'G2'], [7, 'H1']]\n",
    "#  \n",
    "for setting_combi in setting_combinations:\n",
    "    args.data_dir = '/home/yunfei/spatial_benchmarking/benchmarking_data/Her2_tumor'\n",
    "    args.de_candidates = \"None\"\n",
    "    dataset = args.input_data = setting_combi[1]\n",
    "    args.cluster_num = setting_combi[0]\n",
    "    args.impute_cluster_num = [setting_combi[0]]\n",
    "    args.radius = 200\n",
    "    args.use_preprocessing = 1\n",
    "    args.use_hvgs = 0\n",
    "    aris = []\n",
    "    \n",
    "    if args.input_data not in ['20180417_BZ5_control', '20180419_BZ9_control', '20180424_BZ14_control', 'STARmap_20180505_BY3_1k.h5ad'] :\n",
    "        filter_num = filter_num_calc(args, args.filter_num)\n",
    "        print(\"optimized filter number = \", filter_num)\n",
    "    else:\n",
    "        filter_num = 0\n",
    "    adata, adata_ori = initialize(args, filter_num)\n",
    "    if args.de_candidates == \"None\":\n",
    "        if os.path.exists('./cache/Her2st' + dataset + '.txt'):\n",
    "            with open('./cache/Her2st' + dataset + '.txt', 'r') as fp:\n",
    "                line = fp.readlines()[0]\n",
    "                split_ = line.strip().split(\",\")\n",
    "                de_top_k_list = []\n",
    "                for e in split_:\n",
    "                    de_top_k_list.append(int(e))\n",
    "            print(\"previously cached de list = \", de_top_k_list)\n",
    "        else:\n",
    "            de_top_k_list = DE_num_calc(args, adata)\n",
    "            print(\"optimized de list = \", de_top_k_list)\n",
    "            with open('./cache/Her2st' + dataset + '.txt', 'a+') as fp:\n",
    "                # fp.write('de list: ')\n",
    "                fp.write(','.join([str(i) for i in de_top_k_list]))\n",
    "                # fp.write('\\n')\n",
    "    else:\n",
    "        split_ = args.de_candidates.strip().split(\",\")\n",
    "        de_top_k_list = []\n",
    "        for e in split_:\n",
    "            de_top_k_list.append(int(e))\n",
    "        print(\"manually defined de list = \", de_top_k_list)\n",
    "    \n",
    "\n",
    "    for iter_ in range(iters):\n",
    "        de_list_epoch = []\n",
    "        if de_top_k_list != []:\n",
    "            print(\"performing DEGs selection\")\n",
    "            adata_list = []\n",
    "            for de_ in de_top_k_list:\n",
    "                for cluster_n in args.impute_cluster_num:\n",
    "                    print(\"cluster_n = \", cluster_n)\n",
    "                    GAAE.get_kNN(adata, rad_cutoff=args.radius)\n",
    "\n",
    "                    ari_ini, ari_final, de_list, adata_out = GAAE.train_ADEPT_use_DE(adata, n_epochs=1000,\n",
    "                                                                                num_cluster=int(cluster_n),\n",
    "                                                                                dif_k=de_, device_id=args.use_gpu_id)\n",
    "                    de_list_epoch.append(de_list)\n",
    "                    adata_list.append(adata_out)\n",
    "            g_union = set.union(*de_list_epoch)\n",
    "            imputed_ad = impute(args, adata_list, g_union, de_top_k_list)\n",
    "        else:\n",
    "            print(\"skip performing DEGs selection\")\n",
    "            imputed_ad = adata\n",
    "\n",
    "        \"\"\"result of imputed data\"\"\"\n",
    "        GAAE.get_kNN(imputed_ad, rad_cutoff=args.radius)\n",
    "        ari_ini, ARI, de_list, adata_out = GAAE.train_ADEPT_use_DE(imputed_ad, n_epochs=1000, num_cluster=args.cluster_num, device_id=args.use_gpu_id)\n",
    "\n",
    "        print('Dataset:', dataset)\n",
    "        print('ARI:', ARI)\n",
    "        aris.append(ARI)\n",
    "        print(aris)\n",
    "    print('Dataset:', dataset)\n",
    "    print(aris)\n",
    "    print(np.mean(aris))\n",
    "    with open('adept_aris.txt', 'a+') as fp:\n",
    "        fp.write('Her2tumor' + dataset + ' ')\n",
    "        fp.write(' '.join([str(i) for i in aris]))\n",
    "        fp.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
